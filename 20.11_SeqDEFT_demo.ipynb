{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Density Estimation using Field Theory (SeqDEFT)\n",
    "\n",
    "This notebook demonstrates how to do the following tasks in SeqDEFT:\n",
    "\n",
    "(1) Trace the MAP curve, and find the optimal hyperparameter; <br>\n",
    "(2) Visualize the optimal density; <br>\n",
    "(3) Compute pairwise associations of the optimal density; <br>\n",
    "(4) Perform posterior sampling using Hamiltonian Monte Carlo.\n",
    "\n",
    "This demonstration employs a small example dataset simulated from the prior. Applications to sequence spaces with several hundred thousands of genotypes can be done on a typical laptop computer within days. For larger sequence spaces up to low millions of genotypes, it is strongly recommended to deploy the calculations on a cluster.\n",
    "\n",
    "### Example\n",
    "\n",
    "We consider a biological sequence space formed by short (length = 5) DNA sequences. The dimension of the space is equal to 1,024. These sequences are ordered in the lexicographical order: `AAAAA`, `AAAAC`, `AAAAG`, `AAAAT`, ..., `TTTTT`. The number of occurrence of each sequence will be simulated from the prior.\n",
    "\n",
    "We use the 2nd order association (i.e. log odds ratio) as the \"smoothness\" measure of the density. The corresponding MaxEnt solution is then the independent sites model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Import packages that will be needed later\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "from matplotlib import gridspec\n",
    "from matplotlib.collections import LineCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Import functions of SeqDEFT\n",
    "#\n",
    "\n",
    "from functions import preliminary_preparation, simulate_data_from_prior, \\\n",
    "                      trace_MAP_curve, compute_log_Es, compute_log_Es_bounds, compute_log_Ls, \\\n",
    "                      make_visualization, get_nodes, get_edges, \\\n",
    "                      compute_log_ORs, \\\n",
    "                      posterior_sampling, compute_R_hat, combine_samples, plot_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary preparation\n",
    "\n",
    "Specify the following parameters:\n",
    "\n",
    "`alpha` = number of alleles <br>\n",
    "`l` = sequence length <br>\n",
    "`P` = order of association to compute \n",
    "\n",
    "This will set global parameters, and create/load graph Laplacian and its kernel basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 4\n",
    "l = 5\n",
    "P = 2\n",
    "\n",
    "# ----------\n",
    "\n",
    "preliminary_preparation(alpha, l, P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate data from the prior\n",
    "\n",
    "Specify the following parameters:\n",
    "\n",
    "`N` = number of data <br>\n",
    "`a_true` = assumed value of the hyperparameter \n",
    "\n",
    "This will create a dictionary `data_dict` that contains the number `N` and the frequency `R` of the data, as well as the density `Q_true` generating the data. The product of `N` and `R` gives the number of occurrence of the sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "a_true = 100\n",
    "\n",
    "# ----------\n",
    "\n",
    "data_dict = simulate_data_from_prior(N, a_true, random_seed=0)\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Trace the MAP curve, and find the optimal hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1a) Trace the MAP curve\n",
    "\n",
    "Specify the following parameters:\n",
    "\n",
    "`resolution` = target geodesic distance between two densities <br>\n",
    "`num_a` = number of values of the hyperparameter to compute along the MAP curve\n",
    "\n",
    "This will trace the MAP curve. If `num_a` is specified, `num_a` values of the hyperparameter will be computed, in addition to zero and infinity. Otherwise, the function will find as many values of the hyperparameter as it needs such that any pair of adjacent densities has a geodesic distance less than `resolution`. In either case, `resolution` is also used to find a minimum and a maximum finite value of the hyperparameter. The resulting MAP curve will consist of hyperparameter values between these two extremes. The result will be saved in the dataframe `df_map`.\n",
    "\n",
    "**Note 1.** The computation will take about 2 seconds. <br>\n",
    "**Note 2.** In some cases, when the hyperparameter is very large or very small, the minimization algorithm may return an error message, such as `ABNORMAL_TERMINATION_IN_LNSRCH` or `STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT`. There are (at least) two ways to deal with this situation. (1) We can simply discard these extreme values of the hyperparameter, as long as this does not affect the determination of the optimal hyperparameter. (2) We can make the numerical convergence criterion less strict by increasing `gtol` in `options`, as long as the (premature) solution is good enough. The MAP estimate of the field, denoted $\\phi^a$, must satisfy the following equation:\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^G e^{-\\phi^a_i} = 1\n",
    "\\end{equation}\n",
    "where $G$ is the dimension of the space. This equation holds for any value of the hyperparameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "resolution = 0.1\n",
    "num_a = 20\n",
    "\n",
    "# ----------\n",
    "\n",
    "df_map = trace_MAP_curve(data_dict, resolution, num_a, options={'ftol':1e-100, 'gtol':1e-3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1b) Compute Bayesian evidence ratio and its bounds\n",
    "\n",
    "This will compute Bayesian evidence ratio (which is suitable for small problems only) and its lower and upper bounds. The result will be saved in the dataframe `df_map`.\n",
    "\n",
    "**Note.** The computation of Bayesian evidence ratio will take about 55 seconds, and the computation of the bounds will take less than 1 second. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map = compute_log_Es(data_dict, df_map)\n",
    "df_map = compute_log_Es_bounds(data_dict, df_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Make plot\n",
    "#\n",
    "\n",
    "aa = df_map['a'].values\n",
    "log_Es = df_map['log_E'].values\n",
    "log_Es_lb, log_Es_ub = df_map['log_E_lb'].values, df_map['log_E_ub'].values\n",
    "\n",
    "a_star = aa[log_Es.argmax()]\n",
    "max_log_E = log_Es.max()\n",
    "\n",
    "# ----------\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "\n",
    "with np.errstate(divide='ignore'):\n",
    "    plt.fill_between(np.log10(aa), log_Es_lb, log_Es_ub, color='orange', alpha=0.5, zorder=1)\n",
    "    plt.hlines(0, xmin=-4, xmax=9, color='grey', linestyles='--', zorder=2)\n",
    "    plt.scatter(np.log10(aa), log_Es, color='blue', s=15, zorder=3)\n",
    "    plt.scatter(9, log_Es[-1], color='blue', s=15, zorder=3)\n",
    "    plt.scatter(np.log10(a_star), max_log_E, color='red', s=15, zorder=4)\n",
    "    \n",
    "plt.annotate(s=('a* = %.1f'%a_star), xy=(5, 7900), fontsize=14)\n",
    "\n",
    "plt.xlim(-4, 9)\n",
    "plt.ylim(-1000, 9000)\n",
    "plt.xticks([-4, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 9], [r'$-\\infty$','-2','-1','0','1','2','3','4','5','6','7',r'$\\infty$'])\n",
    "plt.xlabel('log10 (a)', fontsize=14)\n",
    "plt.ylabel('log (E)', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1c) Compute cross-validated likelihood\n",
    "\n",
    "Specify the following parameters:\n",
    "\n",
    "`cv_fold` = number of partitions of the data\n",
    "\n",
    "This will compute data likelihood by doing `cv_fold`-fold cross validation. The result will be saved in the dataframe `df_map`.\n",
    "\n",
    "**Note 1.** The computation will take about 6 seconds. <br>\n",
    "**Note 2.** See Note 2 in (1a) for details about the numerical solutions and related issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv_fold = 5\n",
    "\n",
    "# ----------\n",
    "\n",
    "df_map = compute_log_Ls(data_dict, df_map, cv_fold, random_seed=0, options={'ftol':1e-100, 'gtol':1e-3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Make plot\n",
    "#\n",
    "\n",
    "aa = df_map['a'].values\n",
    "log_Ls = df_map['log_L'].values\n",
    "\n",
    "a_star = aa[log_Ls.argmax()]\n",
    "max_log_L = log_Ls.max()\n",
    "\n",
    "# ----------\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "\n",
    "with np.errstate(divide='ignore'):\n",
    "    plt.scatter(np.log10(aa), log_Ls, color='blue', s=15, zorder=1)\n",
    "    plt.scatter(9, log_Ls[-1], color='blue', s=15, zorder=1)\n",
    "    plt.scatter(np.log10(a_star), max_log_L, color='red', s=15, zorder=2)\n",
    "\n",
    "plt.annotate(s=('a* = %.1f'%a_star), xy=(5, -12250), fontsize=14)\n",
    "\n",
    "plt.xlim(-4, 9)\n",
    "plt.ylim(-14000, -12000)\n",
    "plt.xticks([-4, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 9], [r'$-\\infty$','-2','-1','0','1','2','3','4','5','6','7',r'$\\infty$'])\n",
    "plt.xlabel('log10 (a)', fontsize=14)\n",
    "plt.ylabel('log (L)', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1d) Determine the optimal hyperparameter, and compare the MAP estimate with the true distribution\n",
    "\n",
    "We use the more general cross-validated likelihood to determine the optimal hyperparameter and the corresponding field and density. Then we compare the corresponding MAP estimate to the true distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = df_map['a'].values\n",
    "phis = df_map['phi'].values\n",
    "log_Ls = df_map['log_L'].values\n",
    "\n",
    "a_star = aa[log_Ls.argmax()]\n",
    "phi_star = phis[log_Ls.argmax()]\n",
    "Q_star = np.exp(-phi_star) / np.sum(np.exp(-phi_star))\n",
    "\n",
    "print('a_true = %.1f' % a_true)\n",
    "print('a_star = %.1f' % a_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Make plot\n",
    "#\n",
    "\n",
    "Q_true = data_dict['Q_true']\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot([-6,0], [-6,0], color='grey', linewidth=0.5, alpha=0.5, zorder=1)\n",
    "plt.scatter(np.log10(Q_true), np.log10(Q_star), color='black', s=5, alpha=0.4, zorder=2)\n",
    "plt.xlim(-5.5, -1)\n",
    "plt.ylim(-5.5, -1)\n",
    "plt.xlabel('Q_true', fontsize=14)\n",
    "plt.ylabel('Q*', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Visualize the optimal density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2a) Compute visualization coordinates\n",
    "\n",
    "Specify the following parameters:\n",
    "\n",
    "`markov_chain` = model that will be used to build transition matrix of the Markov chain <br>\n",
    "`K` = number of visualization coordinates\n",
    "\n",
    "This will compute `K` visualization coordinates of the optimal density with the `markov_chain` model. The result will be saved in the dataframe `df_visual`. The first visualization coordinate is a constant vector, and hence, will not be used.\n",
    "\n",
    "**Note.** The computation will take about 1 second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_chain = 'evolutionary'\n",
    "K = 20\n",
    "\n",
    "# ----------\n",
    "\n",
    "df_visual, _ = make_visualization(Q_star, markov_chain, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2b) Plot spectrum of the transition matrix\n",
    "\n",
    "Plot reciprocal eigenvalue spectrum of the transition matrix. The reciprocal of an eigenvalue is proportional to the fraction of variation explained in that direction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Make plot\n",
    "#\n",
    "\n",
    "x = range(1, K)\n",
    "y = 1 / abs(df_visual['eigenvalue'].values[1:])\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(x, y, color='blue', s=15)\n",
    "plt.xlim(0,)\n",
    "plt.xticks([0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20])\n",
    "plt.xlabel('k', fontsize=14)\n",
    "plt.ylabel('1 / |Eigenvalue|', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2c) Plot visualization of the optimal density\n",
    "\n",
    "Specify the following parameters:\n",
    "\n",
    "`kx` = the visualization coordinate to be plotted on x-axis <br>\n",
    "`ky` = the visualization coordinate to be plotted on y-axis <br>\n",
    "\n",
    "This will get the nodes and edges of the Hamming graph first, and then plot them in the figure. Each node is colored according to its density. The squared distance between two nodes in the figure is proportional to the commute time between these two nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Make plot\n",
    "#\n",
    "\n",
    "kx = 1\n",
    "ky = 2\n",
    "\n",
    "# ----------\n",
    "\n",
    "df_nodes = get_nodes(df_visual, kx, ky)\n",
    "df_edges = get_edges(df_visual, kx, ky)\n",
    "\n",
    "plt.figure(figsize=(9.5,8))\n",
    "\n",
    "edges = df_edges['edge'].values\n",
    "ln_coll = LineCollection(edges, color='grey', linewidths=0.5, alpha=0.5, zorder=1)\n",
    "ax = plt.gca()\n",
    "ax.add_collection(ln_coll)\n",
    "plt.draw()\n",
    "\n",
    "nodes_x, nodes_y, nodes_c = df_nodes['x'].values, df_nodes['y'].values, np.log10(Q_star)\n",
    "vmin, vmax = np.floor(np.log10(Q_star).min()), np.ceil(np.log10(Q_star).max())\n",
    "cm = plt.cm.get_cmap('coolwarm')\n",
    "sc = plt.scatter(nodes_x, nodes_y, c=nodes_c, vmin=vmin, vmax=vmax, cmap=cm, s=10, zorder=2)\n",
    "plt.colorbar(sc).set_label(label='log10 (Q*)', size=14)\n",
    "\n",
    "plt.xlabel('Diffusion Axis %d'%kx, fontsize=14)\n",
    "plt.ylabel('Diffusion Axis %d'%ky, fontsize=14)            \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Compute pairwise associations of the optimal density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3a) Specify mutations at each site\n",
    "\n",
    "Here we simply generate mutations at each site alphabetically, and save the result in the dictionary `mutations_dict`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bases = ['A','C','G','T']\n",
    "\n",
    "mutations_dict = {}\n",
    "for i in range(l):\n",
    "    mutations_dict[i] = list(itertools.combinations(bases, 2))\n",
    "mutations_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3b) Compute log odds ratios and their rms values\n",
    "\n",
    "For every pair of sites, compute log odds ratios for all possible mutations at the two sites.\n",
    "\n",
    "**Note.** The computation will take about 5 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_mut = int(alpha*(alpha-1)/2)\n",
    "\n",
    "log_ORs_dict, rms_log_ORs = {}, np.zeros([l*num_mut,l*num_mut])\n",
    "for i in range(l):\n",
    "    for j in range(l):\n",
    "        if i == j:\n",
    "            square_ij = np.zeros([num_mut,num_mut])\n",
    "            for m in range(num_mut):\n",
    "                for n in range(num_mut):\n",
    "                    square_ij[m,n] = np.nan\n",
    "            rms_log_ORs[i*num_mut:(i+1)*num_mut,j*num_mut:(j+1)*num_mut] = square_ij\n",
    "        else:\n",
    "            print('Computing log_ORs with given mutations at site %d & site %d ...'%(i,j))\n",
    "            site_i, site_j = i, j\n",
    "            site_i_muts, site_j_muts = mutations_dict[site_i], mutations_dict[site_j]\n",
    "            dict_ij, k, square_ij = {}, 0, np.zeros([num_mut,num_mut])\n",
    "            for m in range(num_mut):\n",
    "                for n in range(num_mut):\n",
    "                    site_i_mut, site_j_mut = site_i_muts[m], site_j_muts[n]\n",
    "                    df_log_ORs = compute_log_ORs(phi_star, site_i, site_j, site_i_mut, site_j_mut, coding_dict={'A':0,'C':1,'G':2,'T':3})\n",
    "                    log_ORs = df_log_ORs['log_OR'].values\n",
    "                    dict_ij[k] = log_ORs\n",
    "                    square_ij[m,n] = np.sqrt(np.mean(log_ORs**2))\n",
    "                    k += 1\n",
    "            log_ORs_dict[(i,j)] = dict_ij\n",
    "            rms_log_ORs[i*num_mut:(i+1)*num_mut,j*num_mut:(j+1)*num_mut] = square_ij"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3c) Plot effective association magnitudes\n",
    "\n",
    "This will plot effective association magnitudes between any two sites, defined as exponential rms log odds ratios. Need to specify the following parameters:\n",
    "\n",
    "`vmin` = minimum of exponential rms log odds ratios <br>\n",
    "`vmax` = maximum of exponential rms log odds ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Make plot\n",
    "#\n",
    "\n",
    "vmin = 4\n",
    "vmax = 14\n",
    "\n",
    "# ----------\n",
    "\n",
    "labels = ['1','2','3','4','5']\n",
    "ticks = [0.5, 1.5, 2.5, 3.5, 4.5, 5.5]\n",
    "ticklabels_dict = {}\n",
    "for i in range(l):\n",
    "    ticklabels_dict[i] = [mut[0]+' > '+mut[1] for mut in mutations_dict[i]]\n",
    "\n",
    "fig = plt.figure(figsize=(8.5,8))\n",
    "gs = fig.add_gridspec(nrows=1, ncols=2, width_ratios=[10,1], height_ratios=[1])\n",
    "gs0 = gridspec.GridSpecFromSubplotSpec(l, l, subplot_spec=gs[0])\n",
    "gs1 = gridspec.GridSpecFromSubplotSpec(1, 1, subplot_spec=gs[1])\n",
    "\n",
    "for i in range(l):\n",
    "    for j in range(l):\n",
    "        ax0 = fig.add_subplot(gs0[i,j])\n",
    "        sns.heatmap(np.exp(rms_log_ORs[i*num_mut:(i+1)*num_mut,j*num_mut:(j+1)*num_mut]), \n",
    "                    vmin=vmin, vmax=vmax, square=True, annot=False, cmap='Blues', cbar=False)\n",
    "        ax0.set_xticks([])\n",
    "        ax0.set_yticks([])\n",
    "        if i == (l-1):\n",
    "            ax0.set_xticks(ticks=ticks)\n",
    "            ax0.set_xticklabels(labels=ticklabels_dict[j], rotation='vertical') \n",
    "            ax0.set_xlabel('%s'%labels[j], fontsize=14)\n",
    "        if j == 0:\n",
    "            ax0.set_yticks(ticks=ticks)\n",
    "            ax0.set_yticklabels(labels=ticklabels_dict[i], rotation='horizontal')\n",
    "            ax0.set_ylabel('%s'%labels[i], fontsize=14)\n",
    "            \n",
    "plt.subplots_adjust(wspace=0.01, hspace=0.01)\n",
    "\n",
    "ax1 = fig.add_subplot(gs1[0,0])\n",
    "sns.heatmap(np.exp(rms_log_ORs), vmin=vmin, vmax=vmax, square=False, annot=False, cmap='Blues', cbar=False)\n",
    "cbar = ax1.figure.colorbar(ax1.collections[0], fraction=0.5, aspect=30)\n",
    "cbar.set_ticks([4, 6, 8, 10, 12, 14])\n",
    "cbar.set_ticklabels(['4', '6', '8', '10', '12', '14'])\n",
    "cbar.ax.tick_params(labelsize=12) \n",
    "cbar.set_label('Effective Association Magnitude', size=14)\n",
    "plt.gca().set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3d) Plot histograms of log odds ratios\n",
    "\n",
    "Specify the following parameters:\n",
    "\n",
    "`site_i` = one site <br>\n",
    "`site_j` = another site\n",
    "\n",
    "This will plot histograms of log odds ratios for all possible mutations at the two sites `site_i` and `site_j`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Make plot\n",
    "#\n",
    "\n",
    "site_i = 1\n",
    "site_j = 3\n",
    "\n",
    "# ----------\n",
    "\n",
    "labels = ['1','2','3','4','5']\n",
    "ticklabels_dict = {}\n",
    "for i in range(l):\n",
    "    ticklabels_dict[i] = [r'%s$\\rightarrow$%s'%(mut[0],mut[1]) for mut in mutations_dict[i]]\n",
    "\n",
    "fig, axes = plt.subplots(num_mut, num_mut, figsize=(8,8))\n",
    "k = 0\n",
    "for m in range(num_mut):\n",
    "    for n in range(num_mut):\n",
    "        k += 1\n",
    "        log_ORs = log_ORs_dict[(site_i-1,site_j-1)][k-1]\n",
    "        plt.subplot(num_mut, num_mut, k)\n",
    "        plt.hist(log_ORs, bins=15, density=True, color='orange')\n",
    "        plt.ylim(0, 0.55)\n",
    "        plt.xlim(-5, 5)\n",
    "        plt.xticks([-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5], [])\n",
    "        plt.yticks([])\n",
    "        plt.tick_params(axis='x', direction='in')\n",
    "        if m == (num_mut-1):\n",
    "            plt.xlabel('%s'%ticklabels_dict[site_j-1][n], fontsize=14)\n",
    "        if n == 0:\n",
    "            plt.ylabel('%s'%ticklabels_dict[site_i-1][m], fontsize=14)\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "fig.add_subplot(111, frameon=False)\n",
    "plt.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False)\n",
    "plt.xlabel('Site %s'%labels[site_j-1], fontsize=16)\n",
    "plt.ylabel('Site %s'%labels[site_i-1], fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Perform posterior sampling using Hamiltonian Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4a) Run multiple HMC chains with different initial phi\n",
    "\n",
    "Specify the following parameters:\n",
    "\n",
    "`num_chains` = number of HMC chains <br>\n",
    "`num_samples_per_chain` = number of samples per HMC chain <br>\n",
    "`num_warmup` = number of warmup draws <br>\n",
    "`num_thinning` = number of thinning draws <br>\n",
    "`e` = (initial) stepsize <br>\n",
    "`L` = (initial) number of leapfrog steps <br>\n",
    "`Le` = (fixed) product of `L` and `e` <br>\n",
    "`window` = stepsize adaptation window <br>\n",
    "\n",
    "This will run multiple HMC chains with different initial phi. The first `num_warmup` draws will be discarded, and from then on, samples will be picked every `num_thinning` draws. The stepsize `e` will be automatically adapted according to the acceptance rate in each period made of `window` draws. (For HMC, the target acceptance rate is 0.65.) The number of leapfrog steps `L` will be adapted accordingly, such that the product `Le` remains the same. The result will be saved in the 3D numpy array `multi_phi_samples`.\n",
    "\n",
    "**Note.** The computation will take about 12 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_chains = 10\n",
    "num_samples_per_chain = 100\n",
    "num_warmup = 1000\n",
    "num_thinning = 10\n",
    "e = 0.2\n",
    "L = 5\n",
    "Le = 1\n",
    "window = 100\n",
    "\n",
    "# ----------\n",
    "\n",
    "method = 'hmc'\n",
    "args = {'e': e, \n",
    "        'L': L,\n",
    "        'Le': Le,\n",
    "        'L_max': np.inf,\n",
    "        'm': 1,\n",
    "        'f': 0,\n",
    "        'window': window,\n",
    "        'gamma_old': 1,\n",
    "        'gamma_new': 1,\n",
    "        'perturbation': 2,\n",
    "        'num_warmup': num_warmup,\n",
    "        'num_thinning': num_thinning}\n",
    "seeds = range(num_chains)\n",
    "\n",
    "multi_phi_samples = np.zeros([num_chains, alpha**l, num_samples_per_chain])\n",
    "for k in range(num_chains):    \n",
    "    print('Running HMC chain # %d ...' % k)\n",
    "    phi_initial, phi_samples, acceptance_rates = \\\n",
    "        posterior_sampling(phi_star, a_star, data_dict, num_samples_per_chain, method, args, random_seed=seeds[k])\n",
    "    multi_phi_samples[k,:,:] = phi_samples\n",
    "    print('acceptance_rates in the sampling phase per window =', acceptance_rates[10:])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4b) Compute R_hat for each component of phi\n",
    "\n",
    "If the HMC chains are well mixed, the \"potential scale reduction factors\" `R_hats` of all the components of phi should be around 1. If this is not the case, redo (4a) with a larger value of `Le`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_hats = compute_R_hat(multi_phi_samples)\n",
    "print('R_hat: %.3f (# %d) ~ %.3f (# %d)' % (R_hats.min(), R_hats.argmin(), R_hats.max(), R_hats.argmax()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4c) Combine all samples\n",
    "\n",
    "If satisfied with the samples, turn the 3D numpy array `multi_phi_samples` into a 2D numpy array `phi_samples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_samples = combine_samples(multi_phi_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4d) Plot distribution of the i-th component of phi\n",
    "\n",
    "Specify the following parameters:\n",
    "\n",
    "`ii` = list of components of phi \n",
    "\n",
    "This will plot marginal distribution of these `ii` components of phi, along with the MAP estimate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = [0, 1, 2]\n",
    "\n",
    "# ----------\n",
    "\n",
    "phi_samples_list = [phi_samples]\n",
    "phi_map = phi_star\n",
    "num_bins = 15\n",
    "colors=['blue']\n",
    "\n",
    "for i in ii:\n",
    "    plot_distribution(i, phi_samples_list, phi_map, num_bins, colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
